{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJf8YzGko5IX",
        "outputId": "7d9a5b0c-ee87-45c3-d665-d6fe4e9298d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.3.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.47.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.27.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n",
            "Requirement already satisfied: openai==0.27.8 in /usr/local/lib/python3.11/dist-packages (0.27.8)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.27.8) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.27.8) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.27.8) (3.11.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.27.8) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.27.8) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.27.8) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.27.8) (2024.12.14)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.27.8) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.27.8) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.27.8) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.27.8) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.27.8) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.27.8) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.27.8) (1.18.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "# Hugging Face Transformers ve Sentence-BERT için\n",
        "!pip install transformers\n",
        "!pip install sentence-transformers\n",
        "\n",
        "# Graph tabanlı işlemler için NetworkX\n",
        "!pip install networkx\n",
        "\n",
        "# OpenAI API için\n",
        "!pip install openai==0.27.8\n",
        "\n",
        "\n",
        "# Veri işleme için Pandas ve NumPy\n",
        "!pip install pandas numpy\n",
        "\n",
        "# BLEU, F1 gibi metrikleri hesaplamak için scikit-learn\n",
        "!pip install scikit-learn\n",
        "\n",
        "# Gerekirse veri temizleme için NLTK\n",
        "!pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQHC3I9WpTzh",
        "outputId": "de16ab93-2eb8-44fe-c76c-8e0ac3f9b33a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding boyutu: 384\n",
            "Embedding örneği: [-0.10186581  0.26830584 -0.0162548   0.09365004 -0.05718497]\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Embedding modeli yükleme\n",
        "embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "# Basit test\n",
        "test_sentence = \"Merhaba! Bu, embedding testidir.\"\n",
        "embedding = embedding_model.encode(test_sentence)\n",
        "\n",
        "print(f\"Embedding boyutu: {len(embedding)}\")\n",
        "print(f\"Embedding örneği: {embedding[:5]}\")  # İlk 5 değerini yazdırma\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKSYm1znpZ7k"
      },
      "outputs": [],
      "source": [
        "# Modülleri import etme\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import openai\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "\n",
        "# Veri setini yükleme\n",
        "file_path = \"/content/sample_data/final_train_data_v2.json\"  # JSON dosyanızın yolu\n",
        "data = pd.read_json(file_path)\n",
        "\n",
        "# Veri setindeki bağlamları ve soruları ayıklama\n",
        "contexts = []\n",
        "qas = []\n",
        "\n",
        "for entry in data[\"data\"]:\n",
        "    for paragraph in entry[\"paragraphs\"]:\n",
        "        context = paragraph[\"context\"]\n",
        "        contexts.append(context)\n",
        "        for qa in paragraph[\"qas\"]:\n",
        "            qas.append({\"question\": qa[\"question\"], \"answer\": qa[\"answers\"][0][\"text\"], \"context\": context})\n",
        "\n",
        "# DataFrame oluşturma\n",
        "qa_df = pd.DataFrame(qas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFaxpdEDrkvB",
        "outputId": "9eec749a-e8ab-4090-b7b1-9f34e7bb4005"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding dosyaları mevcut, yükleniyor...\n"
          ]
        }
      ],
      "source": [
        "embedding_file = \"context_embeddings.npy\"\n",
        "context_file = \"contexts.npy\"\n",
        "\n",
        "# Embedding işlemini kontrol et ve yükle/kaydet\n",
        "if os.path.exists(embedding_file) and os.path.exists(context_file):\n",
        "    print(\"Embedding dosyaları mevcut, yükleniyor...\")\n",
        "    context_embeddings = np.load(embedding_file)\n",
        "    contexts = np.load(context_file, allow_pickle=True).tolist()\n",
        "else:\n",
        "    print(\"Embedding dosyaları bulunamadı, oluşturuluyor...\")\n",
        "    context_embeddings = embedding_model.encode(contexts, convert_to_tensor=False)\n",
        "    np.save(embedding_file, context_embeddings)\n",
        "    np.save(context_file, contexts)\n",
        "    print(f\"Embedding'ler {embedding_file} ve {context_file} dosyalarına kaydedildi.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "c7m_jb7zrUN2"
      },
      "outputs": [],
      "source": [
        "# Graph tabanlı yapı oluşturma\n",
        "graph = nx.Graph()\n",
        "\n",
        "# Bağlamları düğüm olarak ekleme\n",
        "for i, context in enumerate(contexts):\n",
        "    graph.add_node(i, context=context, embedding=context_embeddings[i])\n",
        "\n",
        "# Bağlamlar arasındaki benzerliklere dayalı olarak kenarları ekleme\n",
        "similarity_threshold = 0.7  # Eşik değer\n",
        "for i in range(len(contexts)):\n",
        "    for j in range(i + 1, len(contexts)):\n",
        "        similarity = cosine_similarity(\n",
        "            context_embeddings[i].reshape(1, -1),\n",
        "            context_embeddings[j].reshape(1, -1),\n",
        "        )[0][0]\n",
        "        if similarity > similarity_threshold:\n",
        "            graph.add_edge(i, j, weight=similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1hcZgEgjtRCL"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "# OpenAI API ayarları\n",
        "openai.api_key = \"\"\n",
        "\n",
        "# Graph tabanlı retriever ve GPT entegrasyonu\n",
        "def query_system(query):\n",
        "    # Sorgu embedding'ini oluşturma\n",
        "    query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
        "\n",
        "    # Graph üzerinden en benzer bağlamı bulma\n",
        "    best_node = None\n",
        "    max_similarity = -1\n",
        "    for node, data in graph.nodes(data=True):\n",
        "        similarity = cosine_similarity(\n",
        "            query_embedding.reshape(1, -1),\n",
        "            data[\"embedding\"].reshape(1, -1),\n",
        "        )[0][0]\n",
        "        if similarity > max_similarity:\n",
        "            max_similarity = similarity\n",
        "            best_node = node\n",
        "\n",
        "    if best_node is not None:\n",
        "        best_context = graph.nodes[best_node][\"context\"]\n",
        "    else:\n",
        "        best_context = \"Bu soruya uygun bir bağlam bulunamadı.\"\n",
        "\n",
        "    # GPT-3.5 ile cevap üretme (Yeni API formatında)\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Kısa ve bağlama uygun cevaplar ver.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Bağlam: {best_context}\\nSoru: {query}\\nCevap:\"}\n",
        "        ],\n",
        "        max_tokens=50,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"].strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTm6Uhn9tnSc",
        "outputId": "1d9e1869-970a-4f63-878d-8d709d16be1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sistem Cevabı: Tesla, 7 Ocak 1943 tarihinde ölmüştür.\n"
          ]
        }
      ],
      "source": [
        "query = \"Tesla hangi yıl ölmüştür?\"\n",
        "print(\"Sistem Cevabı:\", query_system(query))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9tmS0Xj83mx",
        "outputId": "507f40c9-87ae-443f-f3d8-d4f2f407f982"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sistem Cevabı: Tesla, hayatının çoğunu ve emekliliği boyunca bir dizi New York otellerinde yaşamıştır.\n"
          ]
        }
      ],
      "source": [
        "query = \"Tesla hayatının çoğunda nerede yaşamıştır?\"\n",
        "print(\"Sistem Cevabı:\", query_system(query))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ekv_tOE9Iem",
        "outputId": "145505c7-a37b-40a7-c2f9-bc19bde6ae4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sistem Cevabı: Guglielmo Marconi'nin ilk transatlantik radyo yayını gösterisi 1901 yılında gerçekleşti.\n"
          ]
        }
      ],
      "source": [
        "query = \"Marconi'nin radyo gösterisi ne zamandı?\"\n",
        "print(\"Sistem Cevabı:\", query_system(query))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f1pWhsZ9UaZ",
        "outputId": "e04ffc51-997b-4240-9a06-ccbcd9196004"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sistem Cevabı: Luther'in yazmaları Fransa, İngiltere ve İtalya'ya 1519 yılında yayıldı.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "query = \"Luther'in yazmaları Fransa, İngiltere ve İtalya'ya ne zaman yayıldı?\"\n",
        "print(\"Sistem Cevabı:\", query_system(query))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ZrUjUdsg9b7e"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "# OpenAI API ayarları\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "# Graph tabanlı retriever ve GPT entegrasyonu\n",
        "def query_system(query):\n",
        "    # Sorgu embedding'ini oluşturma\n",
        "    query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
        "\n",
        "    # Graph üzerinden en benzer bağlamı bulma\n",
        "    best_node = None\n",
        "    max_similarity = -1\n",
        "    for node, data in graph.nodes(data=True):\n",
        "        similarity = cosine_similarity(\n",
        "            query_embedding.reshape(1, -1),\n",
        "            data[\"embedding\"].reshape(1, -1),\n",
        "        )[0][0]\n",
        "        if similarity > max_similarity:\n",
        "            max_similarity = similarity\n",
        "            best_node = node\n",
        "\n",
        "    if best_node is not None:\n",
        "        best_context = graph.nodes[best_node][\"context\"]\n",
        "    else:\n",
        "        best_context = \"Bu soruya uygun bir bağlam bulunamadı.\"\n",
        "\n",
        "    # GPT-3.5 ile cevap üretme (Yeni API formatında)\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Kısa ve bağlama uygun tek kelimelik cevaplar ver.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Bağlam: {best_context}\\nSoru: {query}\\nCevap:\"}\n",
        "        ],\n",
        "        max_tokens=50,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"].strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPRXDXvp-J_2",
        "outputId": "535a097e-9c97-4760-cdce-0f09c663a92a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sistem Cevabı: 1519\n"
          ]
        }
      ],
      "source": [
        "query = \"Luther'in yazmaları Fransa, İngiltere ve İtalya'ya ne zaman yayıldı?\"\n",
        "print(\"Sistem Cevabı:\", query_system(query))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2TJ_gMtBODy"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from sklearn.metrics import f1_score\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Gerekli kütüphaneleri yükleyin (eğer yüklenmediyse)\n",
        "!pip install nltk\n",
        "!pip install scikit-learn\n",
        "\n",
        "# NLTK ayarları\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Rastgele 10 soru seçme\n",
        "random_questions = qa_df.sample(n=10, random_state=42)\n",
        "\n",
        "# Tahminler ve gerçek cevaplar için liste\n",
        "predictions = []\n",
        "true_answers = []\n",
        "\n",
        "# Tahminleri toplama\n",
        "for idx, row in random_questions.iterrows():\n",
        "    query = row[\"question\"]\n",
        "    true_answer = row[\"answer\"]\n",
        "    true_answers.append(true_answer)\n",
        "\n",
        "    # Sistemin tahmini\n",
        "    predicted_answer = query_system(query)\n",
        "    predictions.append(predicted_answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYIVD0QgBzry",
        "outputId": "d7f74be4-ab4c-415b-db21-e718d8e834db"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /usr/local/share/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /usr/local/share/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     /usr/local/share/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "import os\n",
        "\n",
        "# NLTK veri indirme\n",
        "nltk_data_path = \"/usr/local/share/nltk_data\"\n",
        "if not os.path.exists(nltk_data_path):\n",
        "    os.makedirs(nltk_data_path)\n",
        "\n",
        "nltk.data.path.append(nltk_data_path)\n",
        "\n",
        "# Gerekli verileri indir\n",
        "nltk.download('punkt', download_dir=nltk_data_path)\n",
        "nltk.download('wordnet', download_dir=nltk_data_path)\n",
        "nltk.download('omw-1.4', download_dir=nltk_data_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YR0O7b-bAUXp",
        "outputId": "64b65bcf-323f-40c4-b7fc-92e57808ccdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.0)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-3.1.1 sacrebleu-2.5.1\n",
            "SacreBLEU Skoru: 12.522267138734422\n"
          ]
        }
      ],
      "source": [
        "# SacreBLEU'yu yükleme\n",
        "!pip install sacrebleu\n",
        "\n",
        "# SacreBLEU kullanarak BLEU skorunu hesaplama\n",
        "import sacrebleu\n",
        "\n",
        "\n",
        "# SacreBLEU ile hesaplama\n",
        "bleu_score = sacrebleu.corpus_bleu(predictions, [true_answers])\n",
        "\n",
        "# BLEU skorunu yazdırma\n",
        "print(\"SacreBLEU Skoru:\", bleu_score.score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjZf6JAxDapw",
        "outputId": "1c7dee61-9cdf-496c-ee71-2c90b76a9cab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.11/dist-packages (2.5.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (3.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.0)\n",
            "SacreBLEU Skoru: 12.522267138734422\n",
            "Exact Match Skoru: 0.0\n",
            "Ortalama F1 Skoru: 0.14734848484848484\n"
          ]
        }
      ],
      "source": [
        "# SacreBLEU'yu yükleme\n",
        "!pip install sacrebleu\n",
        "\n",
        "# Gerekli kütüphaneler\n",
        "import sacrebleu\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "\n",
        "# BLEU skoru hesaplama (SacreBLEU ile)\n",
        "bleu_score = sacrebleu.corpus_bleu(predictions, [true_answers])\n",
        "\n",
        "# Exact Match Skoru hesaplama\n",
        "exact_match_score = accuracy_score(true_answers, predictions)\n",
        "\n",
        "# F1 Skoru hesaplama\n",
        "def calculate_f1_score(references, predictions):\n",
        "    f1_scores = []\n",
        "    for ref, pred in zip(references, predictions):\n",
        "        ref_tokens = set(ref.split())\n",
        "        pred_tokens = set(pred.split())\n",
        "        common = ref_tokens & pred_tokens\n",
        "        if len(common) == 0:\n",
        "            f1_scores.append(0)\n",
        "        else:\n",
        "            precision = len(common) / len(pred_tokens)\n",
        "            recall = len(common) / len(ref_tokens)\n",
        "            f1_scores.append(2 * (precision * recall) / (precision + recall))\n",
        "    return sum(f1_scores) / len(f1_scores)\n",
        "\n",
        "average_f1_score = calculate_f1_score(true_answers, predictions)\n",
        "\n",
        "# Sonuçları yazdırma\n",
        "print(\"SacreBLEU Skoru:\", bleu_score.score)\n",
        "print(\"Exact Match Skoru:\", exact_match_score)\n",
        "print(\"Ortalama F1 Skoru:\", average_f1_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVDu5Z4YDkte",
        "outputId": "f9d1f19b-d798-4703-c8c3-62314f3a663f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=b0aefef08fb3b92df272d9fe29c03c3fae994c770bf1d1f1bdd38bf425e5ea2f\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ],
      "source": [
        "# Rouge-score kütüphanesini yükleme\n",
        "!pip install rouge-score\n",
        "\n",
        "# Gerekli modül\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# ROUGE hesaplama fonksiyonu\n",
        "def calculate_rouge(references, predictions):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
        "\n",
        "    for ref, pred in zip(references, predictions):\n",
        "        scores = scorer.score(ref, pred)\n",
        "        rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)\n",
        "        rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)\n",
        "        rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)\n",
        "\n",
        "    # Ortalama ROUGE skorlarını hesaplama\n",
        "    avg_rouge1 = sum(rouge_scores['rouge1']) / len(rouge_scores['rouge1'])\n",
        "    avg_rouge2 = sum(rouge_scores['rouge2']) / len(rouge_scores['rouge2'])\n",
        "    avg_rougeL = sum(rouge_scores['rougeL']) / len(rouge_scores['rougeL'])\n",
        "\n",
        "    return avg_rouge1, avg_rouge2, avg_rougeL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KB7XltU5FUSB",
        "outputId": "e705521b-4b3a-43f1-c3f7-222bc076cc67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ortalama ROUGE-1 Skoru: 0.36384948384948385\n",
            "Ortalama ROUGE-2 Skoru: 0.2331746031746032\n",
            "Ortalama ROUGE-L Skoru: 0.36384948384948385\n"
          ]
        }
      ],
      "source": [
        "# ROUGE skorlarını hesaplama\n",
        "avg_rouge1, avg_rouge2, avg_rougeL = calculate_rouge(true_answers, predictions)\n",
        "\n",
        "# Sonuçları yazdırma\n",
        "print(\"Ortalama ROUGE-1 Skoru:\", avg_rouge1)\n",
        "print(\"Ortalama ROUGE-2 Skoru:\", avg_rouge2)\n",
        "print(\"Ortalama ROUGE-L Skoru:\", avg_rougeL)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
